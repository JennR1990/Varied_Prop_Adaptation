---
title: "Updated_Time_Model"
output: html_notebook
---

I want to take our old two-rate model code and change it so that it takes into account a few different things.

We think there is an attenuation parameter, so a value that reduces the amount of learning
     Reach directions <- (learning algorithm) * AP



```{r}
#combine all reaches and localizations into 1 dataframe
getallparticipants(3)
getPassiveTaps(3)
#get angles they tapped and reached to
GetTapAngles(3)
GetReachAngles(3)
# these codes will baseline the data by target/per participant
BaselineReaches()
BaselineTaps()
```
Lets first look at the reach and localization data averaged across participants across all trials

```{r}
plotvariation()
```


We noticed that people don't fully learn the rotation, and when we varied the rotation size and direction every 12 trials, we noticed a slight decrease in the amount of compensation over time. 

```{r}
Plotlearningovertimebyblock()

```

It would appear that prolonged exposure to changing rotations is decreasing the shift in estimates of hand location and reach deviations.  It would make sense then, to add a parameter to a model that changes based on certain characteristics of the training. It could be that prolonged training, when the rotation is changing constantly, makes adaptation become more explicit, so the CNS becomes resistant to adapting. The two important things to me are:
1. Number of changes
2. Size of Rotation
3. Propriocepitve shift on any given trial

Another question to ask is: what is more important?
1. Reaching the highest asymptote 
2. Reaching a local minimum as fast as possible


how does the proprioceptive shift/variance on a given trial play into it. 

Run decay model on each block of the rotation. see if asymptote and learning rate decrease over time. 

Do a proportion, so the total number of changes/change.  

Mathematically define number 1 and 2. 

look at 1-rate models like Jonathan


```{r}
reaches<- getreachesformodel(variation_reaches)
reach_par <-
    fitTwoRateReachModel(
      reaches = reaches$meanreaches,
      schedule = reaches$distortion,
      oneTwoRates = 2,
      grid = "restricted",
      checkStability = TRUE
    )
 reach_model <-
    twoRateReachModel(par = reach_par, schedule = reaches$distortion)
 write.csv(reach_model$slow,file = "data/variation_slowprocess.csv", quote = FALSE, row.names = FALSE)
```
 
 Now that we have the reaches, localization and slow process for the data.  We want to look at each rotation chunk as it's own learning period.  We somehow need to run those three lines of code below for each of the chunks. 
```{r}
#These codes run it for a bunch of data and bootstrap it a bunch.  Im not doing that yet.  Just use the specific model code and run it on the data that i feed it.
bootstrapSemiAsymptoticDecayModels()
getAsymptoticDecayParameterCIs()
#getSaturationTrials()

#plotting the data we just worked out
plotLR_Aperblock()


```




```{r Random Codes}

##best way to make the reaches proportional
data$perc<- abs((data$meanreaches/abs(data$distortion))*100)

extremes<- list()
for (i in 1:36){
  extremes[[i]]<-data[i,data[i,] > abs(data[i,33])]
}
extremes<- extremes[which(data[,33] != 0)]
blocks<-which(data[,33] != 0)

badpeeps<-c()
for (i in blocks){
  badpeeps<- c(badpeeps, colnames(extremes[[i]]))
}
bp<- unique(badpeeps)
count<- c()
for (i in 1:length(bp)){
  count<-c(count,sum(badpeeps == bp[i]))
}
badpeoples<- data.frame(bp, count)
```